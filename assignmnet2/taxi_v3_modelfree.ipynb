{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaoUE8NfkltNonc2Ihgv77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangsaeyeon/AAI-Web-Development/blob/main/assignmnet2/taxi_v3_modelfree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q-8JWWl8oa6",
        "outputId": "9b334325-9271-46b9-8de1-f3d0128c753d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 96 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (4.1.1)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1e-3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyVUSAF55QGR",
        "outputId": "610233dd-d232-4e09-9962-c17982c8846d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, Q, mode=\"test_mode\"):\n",
        "        self.Q = Q\n",
        "        self.mode = mode\n",
        "        self.n_actions = 6\n",
        "\n",
        "        self.epsilon = 0 #greedy action  \n",
        "        self.N = defaultdict(lambda: np.zeros(self.n_actions))\n",
        "\n",
        "        if self.mode == 'mc_control': \n",
        "            self.epsilon = 1 \n",
        "            self.gamma = 0.9\n",
        "            self.sample = list()\n",
        "            self.alpha = 0.1\n",
        "            self.k = 1\n",
        "    \n",
        "        if self.mode == 'q_learning':\n",
        "            self.epsilon = 1\n",
        "            self.gamma = 0.9 \n",
        "            self.alpha = 1 \n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"  \n",
        "        #select an action a from state s(e.g. epsilon-greedy) and execute it\n",
        "        if np.random.random() < self.epsilon : \n",
        "            action = np.random.choice(self.n_actions) #exploration \n",
        "        else:\n",
        "            action = np.argmax(self.Q[state]) #exploitation\n",
        "\n",
        "        return action \n",
        "        #return action\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        \"\"\"\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.mode =='mc_control': \n",
        "            '''\n",
        "            sample k_th episode using Policy : {S_1,A_1,R_2,...,S_t} \n",
        "            if done: \n",
        "                for each state S_t and action A_t in the episode:\n",
        "                    N(S,A) <- N(S,A) + 1\n",
        "                    Gt = R_t+1 + r*R_t+2 + ... + r^T-1*R_terminal\n",
        "                    Q(S,A) <- Q(S,A) + 1/N(S,A)*(Gt - Q(S,A)) \n",
        "                epsilon <- 1/k \n",
        "                policy <- epsilon-greedy(Q)\n",
        "            '''\n",
        "            \n",
        "            self.sample.append([state, action, reward])\n",
        "            if done:   \n",
        "                G = defaultdict(lambda: np.zeros(self.n_actions))\n",
        "                for state, action, reward in reversed(self.sample): \n",
        "                    self.N[state][action] += 1 \n",
        "                    G[state][action] = reward + self.gamma * G[state][action] \n",
        "                    self.Q[state][action] += self.alpha*((1/self.N[state][action])*(G[state][action] - self.Q[state][action])) \n",
        "                self.epsilon = 1 / self.k\n",
        "                self.k += 0.001 \n",
        "                self.sample = list()\n",
        "       \n",
        "        if self.mode == 'q_learning':\n",
        "            #Q(S,A) <- Q(S,A) + alpha[R + gamma*maxQ(S’,a) - Q(S,A)] \n",
        "            self.Q[state][action] =  self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state]) - self.Q[state][action])\n",
        "            self.epsilon *= 0.99 \n",
        "\n"
      ],
      "metadata": {
        "id": "8NWEnTmSAv4u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "from collections import deque\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "#from agent import Agent\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(\"Action Space\", env.action_space.n)\n",
        "print(\"State Space\", env.observation_space.n)\n",
        "\n",
        "def testing_without_learning():\n",
        "    state = env.reset()\n",
        "    total_rewards = 0\n",
        "\n",
        "    def decode(i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        return reversed(out)\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        print(list(decode(state)))\n",
        "        print(\"0:down, 1:up, 2:right, 3:left, 4:pick, 5:dropoff\")\n",
        "        action = int(input(\"select action: \"))\n",
        "        while action not in [0,1,2,3,4,5]:\n",
        "            action = int(input(\"select action: \"))\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        print(\"reward:\", reward)\n",
        "        total_rewards = total_rewards + reward\n",
        "        if done:\n",
        "            print(\"total reward:\", total_rewards)\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "\n",
        "def model_free_RL(Q, mode):\n",
        "    agent = Agent(Q, mode)\n",
        "    num_episodes = 100000\n",
        "    last_100_episode_rewards = deque(maxlen=100)\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            episode_rewards += reward\n",
        "            if done:\n",
        "                last_100_episode_rewards.append(episode_rewards)\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if (i_episode >= 100):\n",
        "            last_100_episode_rewards.append(episode_rewards)\n",
        "            avg_reward = sum(last_100_episode_rewards) / len(last_100_episode_rewards)\n",
        "            print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, avg_reward), end=\"\")\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "def testing_after_learning(Q, mode):\n",
        "    agent = Agent(Q, mode)\n",
        "    n_tests = 100 \n",
        "    total_test_rewards = []\n",
        "    for episode in range(n_tests):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "    \n",
        "\n",
        "            if done:\n",
        "                total_test_rewards.append(episode_reward)\n",
        "                break\n",
        "\n",
        "            state = new_state\n",
        "    print(total_test_rewards)\n",
        "    print(\"avg: \" + str(sum(total_test_rewards) / n_tests))\n",
        "\n",
        "\n",
        "Q = defaultdict(lambda: np.zeros(action_size))\n",
        "while True:\n",
        "    print()\n",
        "    print(\"1. testing without learning\")\n",
        "    print(\"2. MC-control\")\n",
        "    print(\"3. q-learning\")\n",
        "    print(\"4. testing after learning\")\n",
        "    print(\"5. exit\")\n",
        "    menu = int(input(\"select: \"))\n",
        "    if menu == 1:\n",
        "        testing_without_learning()\n",
        "    elif menu == 2:\n",
        "        Q = defaultdict(lambda: np.zeros(action_size))\n",
        "        model_free_RL(Q, \"mc_control\")\n",
        "    elif menu == 3:\n",
        "        Q = defaultdict(lambda: np.zeros(action_size))\n",
        "        model_free_RL(Q, \"q_learning\")\n",
        "    elif menu == 4:\n",
        "        testing_after_learning(Q, \"test_mode\")\n",
        "    elif menu == 5:\n",
        "        break\n",
        "    else:\n",
        "        print(\"wrong input!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3PKSt5yAqgL",
        "outputId": "ed619ee8-0de5-48e5-91ff-4e3757f3ec3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space 6\n",
            "State Space 500\n",
            "\n",
            "1. testing without learning\n",
            "2. MC-control\n",
            "3. q-learning\n",
            "4. testing after learning\n",
            "5. exit\n",
            "select: 2\n",
            "Episode 100000/100000 || Best average reward 6.96\n",
            "\n",
            "1. testing without learning\n",
            "2. MC-control\n",
            "3. q-learning\n",
            "4. testing after learning\n",
            "5. exit\n",
            "select: 4\n",
            "[8, 8, 7, 4, 4, 6, 7, 8, 7, 8, 5, 6, 9, 11, 5, 10, 9, 8, 12, 8, 8, 8, 7, 7, 9, 10, 5, 9, 5, 13, 8, 5, 7, 5, 4, 13, 5, 3, 8, 10, 3, 6, 10, 7, 1, 7, 8, 3, 4, 10, 11, 9, 10, 4, 14, 6, 5, 1, 9, 8, 4, 4, 7, 5, 9, 2, 8, 8, 9, 7, 12, 5, 6, 5, 0, 11, 6, 11, 10, 6, 7, 13, 7, 4, 7, 9, 6, 13, 8, 3, 4, 9, 5, 1, 6, -1, 1, 9, 9, 7]\n",
            "avg: 6.97\n",
            "\n",
            "1. testing without learning\n",
            "2. MC-control\n",
            "3. q-learning\n",
            "4. testing after learning\n",
            "5. exit\n",
            "select: 3\n",
            "Episode 100000/100000 || Best average reward 7.5\n",
            "\n",
            "1. testing without learning\n",
            "2. MC-control\n",
            "3. q-learning\n",
            "4. testing after learning\n",
            "5. exit\n",
            "select: 4\n",
            "[5, 8, 4, 7, 12, 6, 5, 13, 3, 7, 10, 5, 9, 5, 11, 6, 9, 7, 7, 8, 8, 4, 6, 6, 10, 7, 7, 11, 11, 5, 12, 6, 7, 7, 11, 3, 11, 7, 3, 8, 4, 10, 11, 12, 6, 10, 12, 5, 13, 6, 4, 10, 7, 3, 7, 5, 4, 9, 4, 7, 6, 9, 11, 4, 10, 5, 4, 6, 9, 6, 8, 5, 6, 7, 7, 7, 8, 4, 8, 15, 9, 11, 15, 7, 9, 8, 9, 10, 6, 12, 9, 11, 6, 9, 9, 4, 9, 7, 9, 8]\n",
            "avg: 7.68\n",
            "\n",
            "1. testing without learning\n",
            "2. MC-control\n",
            "3. q-learning\n",
            "4. testing after learning\n",
            "5. exit\n",
            "select: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw9-I9KFjZsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}